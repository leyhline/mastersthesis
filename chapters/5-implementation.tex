\chapter{Implementation}\label{chap:implementation}

The actual implementation uses the Haskell programming language and related tools. There are adjustments in all three module groups as classified in chapter~\ref{chap:program-structure}. Furthermore, we extend the data types in the fundamental \texttt{Syntax} module with additional expressions and types from section~\ref{sec:cc-expressions}.

\begin{lstlisting}[caption=Extensions to \texttt{Syntax.hs}]
data Exp = ...
  | Cast Exp Type Type  -- M : A => B

data Type = ...
  | TBot  -- bottom type
  | TDyn  -- dynamic type *
\end{lstlisting}

\section{Parsing}

As usual, there is a tokenization step before the actual parsing, turning a string into tokens. For this purpose, we use the tool \emph{Alex: A lexical analyser generator for Haskell}\footnote{\url{https://www.haskell.org/alex/}}, maintained by Simon Marlow, as it nicely integrates with Haskell. We add tokens for the bottom type and the the double arrow used in cast expressions. In our implementation, we use \texttt{*} as identifier for the dynamic type. There already exists a corresponding token since it was used for multiplication in the base language.

\begin{lstlisting}[caption=Additional tokens (\texttt{Parsing/Tokens.x})]
data Token = ...
  | TBot
  | DoubleArrow

tokens :-
  ...
  Bot    { tok $ const TBot }
  "_|_"  { tok $ const TBot }
  "=>"   { tok $ const DoubleArrow }
\end{lstlisting}

For parsing, we use \emph{Happy: The Parser Generator for Haskell}\footnote{\url{https://www.haskell.org/happy/}}, also maintained by Simon Marlow. Parsing is slightly more complicated since we need to add the dynamic type $\star$, bottom type and the cast expression $M : A \Rightarrow B$.

\begin{lstlisting}[caption=Extensions to \texttt{Parsing/Grammar.y}]
%token
  ...
  Bot   { T _ T.TBot }
  '=>'  { T _ T.DoubleArrow }

Exp = ...
  | Exp ':' Typ '=>' Typ  { Cast $1 $3 $5 }

ATyp = ...
  | '*'  { TDyn }
  | Bot  { TBot }
\end{lstlisting}

\section{Typechecker}

Since terms involving the newly introduced dynamic type are handled at runtime there is no need for extensive changes to the typechecker. Nevertheless, we need to integrate the new syntax. Otherwise, typechecking would fail, not because of type errors but out of ``ignorance''.

\subsection{Type Synthesis}

In general, type synthesis $\Gamma \vdash M \Rightarrow A;\Delta$ takes environment $\Gamma$ and expression $M$ to synthesize its type $A$ and the environment after $\Delta$. We adjust the corresponding function for the following expressions: (1) application, (2) pair construction and for built-in functions (3) \texttt{fst} and (4) \texttt{snd} (both eliminating pairs, yielding the first or second element). In code, it is complemented by a monad transformer, as seen in its annotation, though this is not relevant here.

\begin{lstlisting}[caption=\texttt{tySynth} function type annotation]
tySynth :: TEnv -> Exp -> TCM (Type, TEnv)

type TCM a =
  WriterT [Constraint] (ExceptT String (State Caches)) a

data Caches = Caches{subCache::Cache, eqvCache::Cache}
type Cache = [(Type, Type)]

data Constraint = Type :<: Type
\end{lstlisting}

For application $M~N$, after synthesizing $\Gamma \vdash M \Rightarrow A;\Delta$ and if $A = \star$, we assert that expression $N$ also has dynamic type $\star$.

\begin{lstlisting}[caption=Application type synthesis (\texttt{TCTyping.hs})]
tySynth te (App e1 e2) = do
  (tf, te1) <- tySynth te e1
  tfu <- unfold te1 tf
  case tfu of
    -- ...
    TDyn -> do
      te2 <- tyCheck te1 e2 TDyn
      return (TDyn, te2)
\end{lstlisting}

An expression involving pairs is ``\lstinline[language=ldgv]!let <x, y> = M in N!'', where $M$ has a pair type. First, the components of $M$ are bound to $x$ and $y$, finally evaluating $N$. For typecheckig, we synthesize type $A$ from $M$. Afterwards, we synthesize $\Delta_A, x:\star, y:\star \vdash N \Rightarrow B$ for the whole expression's type.

\begin{lstlisting}[caption=Pair type synthesis (\texttt{TCTyping.hs})]
tySynth te (LetPair x y e1 e2) = do
  (tp, te1) <- tySynth te e1
  tpu <- unfold te1 tp
  case tpu of
    TPair {} -> -- ...
    TDyn -> tySynth
      ((y, (Many, TDyn)) : (x, (Many, TDyn)) : te1)
      e2
\end{lstlisting}

The built-in functions \texttt{fst} and \texttt{snd} simply return the dynamic type $\star$ upon type synthesis since a pair type $\Sigma(x:\star)\star$ implies that both components are of type $\star$, too.

\subsection{Subtyping}

Subtyping regarding the dynamic type happens in the context of \case, used for eliminating label types. For a term $\case (x:\star \Rightarrow A) \{ \overline{\ell : N_\ell} \}$ we check if type $A$ is a subtype of $N_\ell$. Both are sets of labels, hence $A \leq N_\ell$ if $A \subseteq N_\ell$.

\begin{lstlisting}[caption=Subtyping in case terms (\texttt{TCSubtyping.hs})]
unfold :: TEnv -> Type -> TCM Type
unfold tenv (TCase val@(Var x) cases) = do
  tyx <- varlookupUnfolding x tenv
  let f :: (String, Type) -> TCM Type
      f (lab, labtyp) = unfold
        (("*unfold*", TEqn val (Lit $ LLab lab) tyx) : tenv)
        labtyp
  case tyx of
    TLab {} -> -- ...
    TDyn -> mapM f cases
  where f ::
\end{lstlisting}

\section{Interpreter}

In basic LDGV, types are not relevant in context of evaluation. The interpreter works on the assumption that prior typechecking did succeed. In our extension, cast expressions are always handled at runtime, even if both types in a cast $M : A \Rightarrow B$ are not dynamic ($A \neq \star, B \neq \star$).
For correct handling of the dynamic type and type reduction of cast expressions we introduce new data types and extend the existing data type for values. Finally, we implement the rules of section~\ref{sec:ccldlc-inference-rules}.

In chapter~\ref{chap:cast-calculus} we listed rules for handling singleton types $\Single\{A\}$ as given in the original paper. The implementation does not support explicit singleton types in its grammer. Therefore, from this point onwards, singleton types will not be considered.

\subsection{Type Passing}

Since type reduction requires \emph{head normal form} we introduce an explicit data type \texttt{NFType} in listing~\ref{lst:hnf-typedef}. Label type $L$ is implemented as a set of strings. Similarities between function type $(\rho, \Pi(x:A)B)$ and pair type $(\rho, \Sigma(x:A)B)$ are  depicted using the \texttt{FuncType} data type.

\begin{lstlisting}[float,
  label=lst:hnf-typedef,
  caption=HNF type definition (\texttt{ProcessEnvironment.hs})]
data NFType
  = NFBot
  | NFDyn
  | NFFunc FuncType
  | NFPair FuncType
  | NFInt
  | NFDouble
  | NFGType GType

type Label = String
type LabelType = Set Label
data FuncType = FuncType PEnv String Type Type
\end{lstlisting}

The constructor \texttt{NFGType} is a special case. It takes an argument of type \texttt{GType}---the data type implementing \emph{ground types}---which we define in listing~\ref{lst:gtype}. This type dependency depicts that every ground type is also in head normal form---but not the other way around---while making comparison operations between \texttt{NFType} and \texttt{GType} possible. At the same time, defining ground types as a separate data type allows for more precise function signatures. Even so, we can not perfectly model the relationship since Haskell itself is not dependently typed. A function type $(\cdot, \Pi(x:\star)\star)$ can be written as simple \texttt{NFGType GFunc} or as \texttt{NFFunc (FuncType [] "x" TDyn TDyn)}. The latter is semantically correct but must not occur it most contexts. We have to resort to helper function like \texttt{equalsType} and additional logic to resolve such cases.

\begin{lstlisting}[float,
  caption=Ground type definition (\texttt{ProcessEnvironment.hs}),
  label=lst:gtype]
data GType
  = GUnit
  | GLabel LabelType
  | GFunc String
  | GPair String

equalsType :: NFType -> GType -> Bool
equalsType (NFFunc (FuncType _ _ s TDyn TDyn)) (GFunc s') =
  s == s'
equalsType (NFPair (FuncType _ _ s TDyn TDyn)) (GPair s') =
  s == s'
equalsType (NFGType gt1) gt2 = gt1 == gt2
equalsType _ _ = False
\end{lstlisting}

Subtyping checks are implemented in listing~\ref{lst:subtypeable} instroducing typeclass \texttt{Subtypeable} with instances for ground types \texttt{GType}---mostly with trivial definitions---and partially for HNF types \texttt{NFType}. The latter is solely necessary for implementing the \emph{Cast-Dyn-Dyn} rule.

\begin{lstlisting}[float,
  label=lst:subtypeable,
  caption=Subtypeable typeclass (\texttt{ProcessEnvironment.hs})]
class Subtypeable t where
  isSubtypeOf :: t -> t -> Bool

instance Subtypeable GType where
  isSubtypeOf GUnit GUnit = True
  isSubtypeOf (GLabel ls1) (GLabel ls2) =
    ls1 `Set.isSubsetOf` ls2
  isSubtypeOf (GFunc _) (GFunc _) = True
  isSubtypeOf (GPair _) (GPair _) = True
  isSubtypeOf _ _ = False

instance Subtypeable NFType where
  isSubtypeOf NFBot _ = True
  isSubtypeOf NFDyn NFDyn = True
  -- ...
  isSubtypeOf _ _ = False
\end{lstlisting}

We also extend the value type according to section~\ref{sec:cc-expressions}. We add function closures, including environment $\rho$, variable name and the expression in the function's body. Furthermore function casts and the dynamic wrapper $V : G \Rightarrow \star$.

\begin{lstlisting}[caption=Value type extensions (\texttt{ProcessEnvironment.hs})]
data Value = -- ...
  | VFunc PEnv String Exp
  | VFuncCast Value FuncType FuncType
  | VDynCast Value GType
\end{lstlisting}

We use these new and extended data types to handle CCLDLC's types during the interpretation state, especially for type inference in cast expressions. At last, we need a mapping to HNF types used in type casts. We introduce the \texttt{evalType} function in listing~\ref{lst:type-eval} to evaluate types to their HNF form. Mostly, this evaluation is trivial. For function and pair types, the environment is made part of the type. Most involved is evaluation of the $\case x~\{\ell_i : A_i\}$ type since case types are never in HNF. We match label $x \downarrow \ell_j$ on the type mapping, finally evaluating $A_j \downarrow A^\circ$. If possible, we evaluate to ground types and wrap them using the \texttt{NFGType} constructor.

\begin{lstlisting}[float,
  caption=Type evaluation (\texttt{Interpreter.hs}),
  label=lst:type-eval]
evalType :: Type -> InterpretM NFType
evalType = \case
  TUnit   -> return $ NFGType GUnit
  TInt    -> return NFInt
  TDouble -> return NFDouble
  TBot    -> return NFBot
  TDyn    -> return NFDyn
  TLab ls -> return $ NFGType $ GLabel ls
  TFun  _ _ TDyn TDyn -> return $ NFGType GFunc
  TFun  _ s t1 t2 -> do
    env <- ask
    return $ NFFunc $ FuncType env s t1 t2
  TPair _ _ TDyn TDyn -> return $ NFGType GPair
  TPair _ s t1 t2 -> do
    env <- ask
    return $ NFPair $ FuncType env s t1 t2
  TCase exp labels -> interpret' exp >>= \(VLabel l) ->
    let entry = find (\(l', _) -> l == l') labels
    in maybe (return NFBot) (evalType . snd) entry
\end{lstlisting}

\subsection{Cast Reduction}

The goal of type inference at runtime is to either reduce casts in a way that evaluation halts at a value, or to abort evaluation with \blame. In our implementation, the latter throws an exception. During this section we describe the implementation of the rules stated in figure~\ref{fig:ccldlc-extensions-values}.

We extended the language with cast expression $M : A \Rightarrow B$. However, a cast reduction has to have form $V : A^\circ \Rightarrow B^\circ$. Hence, we use rule \textsc{Cast-Reduce} to evaluate expression $M$ to value $V$ and types $A$ and $B$ to HNF $A^\circ$ and $B^\circ$ respectively. For that purpose, we extend the existing \texttt{eval} function according to listing~\ref{lst:cast-reduce}. This function implements the interpreter's main logic, mapping all the language's expressions (see figure~\ref{fig:ldlc-expressions}) to values. The \texttt{InterpretM} monad from the function's signature represents evaluation environment $\rho$, providing the \emph{Reader} monad's interface.\footnote{\url{https://hackage.haskell.org/package/transformers-0.6.0.4/docs/Control-Monad-Trans-Reader.html}}

\begin{lstlisting}[float,
  caption=Cast expression evaluation (\texttt{Interpreter.hs}),
  label=lst:cast-reduce]
eval :: Exp -> InterpretM Value
eval cast@(Cast e t1 t2) = do
  v <- interpret' e
  nft1 <- evalType t1
  nft2 <- evalType t2
  maybe (blame cast) return (reduceCast v nft1 nft2)
\end{lstlisting}

Cast reduction $V : A^\circ \Rightarrow B^\circ \Downarrow V'$ is implemented in the \texttt{reduceCast} function. Its type signature matches the term as it requires a value and two types in HNF. However, upon failure it will not return \blame directly but encapsulates its result into a \texttt{Maybe} type. Then, the calling function needs to call \blame upon an empty result (e.g. see listing~\ref{lst:cast-reduce}).

The basic \textsc{Cast-Is-Value} rule returns $V : A^\circ \Rightarrow B^\circ$ as it is if it already forms a value. This occurs for $A^\circ = G, B^\circ = \star$ (\texttt{VDynCast}) or for function casts, i.e. $A^\circ=(\rho, \Pi(x:A)A'), B^\circ=(\rho, \Pi(x:B)B')$ (\texttt{VFuncCast}). When implementing the latter we have to consider the ground type \texttt{GFunc} that is also a valid function type but is on a different level in the type hierarchy. Therefore, more code is necessary to handle conversion. If the \textsc{Cast-Is-Value} rule does not apply we check the remaining rules via distinct function \texttt{reduceCast'} as \emph{alternative} (indicated by the \texttt{<|>} operator). The code is given in listing~\ref{lst:cast-is-value}.

\begin{lstlisting}[float,
  caption=Rule \textsc{Cast-Is-Value} (\texttt{Interpreter.hs}),
  label=lst:cast-is-value]
reduceCast :: Value -> NFType -> NFType -> Maybe Value
reduceCast v t1 t2 = castIsValue v t1 t2
                 <|> reduceCast' v t1 t2

castIsValue :: Value -> NFType -> NFType -> Maybe Value
castIsValue v (NFGType gt) NFDyn = Just $ VDynCast v gt
castIsValue v (NFFunc ft1) (NFFunc ft2) =
  Just $ VFuncCast v ft1 ft2
castIsValue v (NFFunc ft1) (NFGType (GFunc y)) =
  Just $ VFuncCast v
    ft1 (FuncType [] y TDyn TDyn)
castIsValue v (NFGType (GFunc x)) (NFFunc ft2) =
  Just $ VFuncCast v
    (FuncType [] x TDyn TDyn) ft2
castIsValue v (NFGType (GFunc x)) (NFGType (GFunc y)) =
  Just $ VFuncCast v
    (FuncType [] x TDyn TDyn) (FuncType [] y TDyn TDyn)
castIsValue _ _ _ = Nothing
\end{lstlisting}

For some rules we match HNF types to ground types as defined in figure~\ref{fig:ccldlc-type-matching}. The actual code of listing~\ref{lst:type-matching} is more concise that the definition because of the way we encapsulated \texttt{GType} and since we do not consider singleton types.

\begin{lstlisting}[float,
  caption=Type matching $A^\circ \rhd G$ (\texttt{Interpreter.hs}),
  label=lst:type-matching]
matchType :: NFType -> Maybe GType
matchType = \case
  NFFunc (FuncType _ _ x _ _) -> Just $ GFunc x
  NFPair (FuncType _ _ x _ _) -> Just $ GPair x
  NFGType gt -> Just gt
  _ -> Nothing
\end{lstlisting}

The remaining rules necessary for a full implementation of CCLDLC , except these for function and pair handling, are handled by the \texttt{reduceCast'} function we describe in the next paragraphs.

Rules \textsc{Cast-Dyn-Dyn} and \textsc{Factor-Left} are similar such that they presume a cast \emph{to} dynamic type $\star$. Hence, we implement them together as in listing~\ref{lst:cast-dyn-dyn-factor-left}. Currently, the only subtypes of $\star$ are $\star$ itself and bottom type $\bot$ which is expressed using typeclass \texttt{Subtypeable} (listing~\ref{lst:subtypeable}). If the given type is no subtype of $\star$ we apply \textsc{Factor-Left} as is. After type matching $A^\circ \rhd G$ we do not implement the check that $A^\circ \neq G$. Given the function's arguments this implies a cast $V : G \Rightarrow \star$ that would have been handled by an earlier call to \texttt{castIsValue}.

\begin{lstlisting}[float,
  label=lst:cast-dyn-dyn-factor-left,
  caption=Rules \textsc{Cast-Dyn-Dyn} and \textsc{Factor-Left} (\texttt{Interpreter.hs})]
reduceCast' v t NFDyn =
  if t `isSubtypeOf` NFDyn
  then Just v --Cast-Dyn-Dyn
  else do
    gt <- matchType t
    v' <- reduceCast v t (NFGType gt)
    Just $ VDynCast v' gt
\end{lstlisting}

There is no type that is a subtype of the bottom type $\bot$. Hence, a cast to $\bot$ must always results in blame which is expressed in rule \textsc{Cast-Bot}.

\begin{lstlisting}[caption=Rule \textsc{Cast-Bot} (\texttt{Interpreter.hs})]
reduceCast' _ _ NFBot = Nothing
\end{lstlisting}

It is straightforward to implement rules \textsc{Cast-Collapse} and \textsc{Cast-Collide}. Even though dynamic type $\star$ does not support transitivity w.r.t. subtyping, these rules enable transitivity for ground types only. The code is given in listing~\ref{lst:cast-collapse}.

\begin{lstlisting}[float,
  caption=Rules \textsc{Cast-Collapse} and \textsc{Cast-Collide} (\texttt{Interpreter.hs}),
  label=lst:cast-collapse]
reduceCast' (VDynCast v gt1) NFDyn (NFGType gt2) =
  if gt1 `isSubtypeOf` gt2 then Just v else Nothing
\end{lstlisting}

Rule \textsc{Factor-Right} describes a cast from $\star$ to another type (listing~\ref{lst:factor-right}). Its utilization of \emph{type matching} $\rhd$ makes it similar to \textsc{Factor-Left}. A failure results in \blame since rules with similar structure, namely \textsc{Cast-Collapse} and \textsc{Cast-Collide}, were handled before by pattern matching over more specific arguments.

\begin{lstlisting}[float,
  caption=Rule \textsc{Factor-Right} (\texttt{Interpreter.hs}),
  label=lst:factor-right]
reduceCast' v NFDyn t = do
  gt <- matchType t
  let nfgt = NFGType gt
  if not (t `equalsType` gt) then do
    v'  <- reduceCast v NFDyn nfgt
    v'' <- reduceCast v' nfgt t
    Just v''
  else
    Nothing
\end{lstlisting}

Finally, we give a simple implementation for rule \textsc{Cast-Sub} which simply checks subtyping between given ground types. On failure, this simply returns \blame without any additional checks. Also, we completely omit rule \textsc{Cast-Fail}. This is because it is the last rule in context of \texttt{reduceCast'}. There are no cases left to examine so any cast not yet covered must fail.

\begin{lstlisting}[caption=Rule \textsc{Cast-Sub} (\texttt{Interpreter.hs})]
reduceCast' v (NFGType gt1) (NFGType gt2) =
  if gt1 `isSubtypeOf` gt2 then Just v else Nothing
\end{lstlisting}

The last two are about type reductions in context of pairs and functions. First, we implemnt \textsc{Cast-Pair}. For this purpose we make adjustments to the \texttt{eval} function in listing~\ref{lst:cast-reduce} which returns monad \texttt{InterpretM}. This is necessary since we need to consider the expression's environment $\rho$. The type annotation of function \texttt{reduceCast} does not accept arguments capable of representing the environment. After evaluating to $V : A^\circ \Rightarrow B^\circ$ we catch pair values by pattern matching and pass them to function \texttt{reducePairCast} as seen in listing~\ref{lst:pair-cast}.

\begin{lstlisting}[float,
  caption=Rule \textsc{Cast-Pair} (\texttt{Interpreter.hs}),
  label=lst:pair-cast]
eval :: Exp -> InterpretM Value
eval cast@(Cast e t1 t2) = do
  v    <- interpret' e
  nft1 <- evalType t1
  nft2 <- evalType t2
  case (v, nft1, nft2) of
    -- catch pair values
    (pair@VPair {}, from@NFPair {}, to@NFPair {}) -> do
      v' <- lift $ reducePairCast pair from to
      maybe (blame cast) return v'
    -- as before
    _ -> maybe (blame cast) return (reduceCast v nft1 nft2)

reducePairCast
  (VPair v w)
  (NFPair (FuncType env  _ t1  t2))
  (NFPair (FuncType env' _ t1' t2')) = do
    v' <- reduceComponent v (env, t1) (env', t1')
    w' <- reduceComponent w (env, t2) (env', t2')
    return $ liftM2 VPair v' w'

reduceComponent v (env, t) (env', t') = do
  nft  <- R.runReaderT (evalType t)  env,
  nft' <- R.runReaderT (evalType t') env'
  return $ reduceCast v nft nft'
\end{lstlisting}
